# main.py
import cv2
import yaml
import numpy as np # For image manipulation
import os # For checking file path
from pose_estimator import PoseEstimator
from ball_tracker import BallTracker
import time

def load_config(config_path="configs/default_config.yaml"):
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def overlay_image_alpha(img_background, img_overlay, x_offset, y_offset):
    """
    Overlays img_overlay (with alpha channel) onto img_background at (x_offset, y_offset).
    Handles boundary conditions.
    """
    bg_h, bg_w = img_background.shape[:2]
    fg_h, fg_w = img_overlay.shape[:2]

    # If the overlay image has 3 channels (BGR), add an alpha channel
    if img_overlay.shape[2] == 3:
        img_overlay_rgba = cv2.cvtColor(img_overlay, cv2.COLOR_BGR2BGRA)
    else:
        img_overlay_rgba = img_overlay.copy() # Already BGRA

    alpha_mask = img_overlay_rgba[:, :, 3] / 255.0
    rgb_overlay = img_overlay_rgba[:, :, :3]

    # Calculate the region of interest (ROI) on the background
    x1, y1 = int(x_offset), int(y_offset)
    x2, y2 = int(x_offset + fg_w), int(y_offset + fg_h)

    # Clip ROI to background dimensions
    roi_x1_bg, roi_y1_bg = max(0, x1), max(0, y1)
    roi_x2_bg, roi_y2_bg = min(bg_w, x2), min(bg_h, y2)

    # Clip overlay image based on what fits in the background
    fg_x1_clip = max(0, -x1)
    fg_y1_clip = max(0, -y1)
    fg_x2_clip = fg_w - max(0, x2 - bg_w)
    fg_y2_clip = fg_h - max(0, y2 - bg_h)

    # If the ROI is invalid (e.g., image completely off screen), return original background
    if roi_x1_bg >= roi_x2_bg or roi_y1_bg >= roi_y2_bg or \
       fg_x1_clip >= fg_x2_clip or fg_y1_clip >= fg_y2_clip:
        return img_background

    # Get the ROI from the background
    roi_bg = img_background[roi_y1_bg:roi_y2_bg, roi_x1_bg:roi_x2_bg]

    # Crop the foreground and alpha mask to match the valid ROI
    rgb_overlay_cropped = rgb_overlay[fg_y1_clip:fg_y2_clip, fg_x1_clip:fg_x2_clip]
    alpha_mask_cropped = alpha_mask[fg_y1_clip:fg_y2_clip, fg_x1_clip:fg_x2_clip]

    # Ensure cropped overlay matches ROI dimensions if clipping occurred
    if roi_bg.shape[:2] != rgb_overlay_cropped.shape[:2]:
        # This can happen if clipping is severe. It might require more careful resizing or skipping.
        # For simplicity, if dimensions don't match perfectly after clipping, we might skip.
        # Or, one could try to resize rgb_overlay_cropped to roi_bg.shape.
        # Let's try resizing if they are not too different
        if abs(roi_bg.shape[0] - rgb_overlay_cropped.shape[0]) < 5 and \
           abs(roi_bg.shape[1] - rgb_overlay_cropped.shape[1]) < 5:
            rgb_overlay_cropped = cv2.resize(rgb_overlay_cropped, (roi_bg.shape[1], roi_bg.shape[0]))
            alpha_mask_cropped = cv2.resize(alpha_mask_cropped, (roi_bg.shape[1], roi_bg.shape[0]))
        else:
            # print("Warning: ROI and overlay dimensions mismatch significantly after clipping.")
            return img_background # Skip if mismatch is too large

    # Blend
    # Ensure alpha_mask_cropped is broadcastable for element-wise multiplication
    alpha_3ch = cv2.cvtColor(alpha_mask_cropped, cv2.COLOR_GRAY2BGR) if len(alpha_mask_cropped.shape) == 2 else alpha_mask_cropped

    composite = (rgb_overlay_cropped * alpha_3ch) + (roi_bg * (1.0 - alpha_3ch))

    # Place the composite back onto the background image
    img_background_result = img_background.copy()
    img_background_result[roi_y1_bg:roi_y2_bg, roi_x1_bg:roi_x2_bg] = composite.astype(np.uint8)

    return img_background_result


def replace_player_head(display_frame, person_keypoints_list, judy_img, head_scale_factor):
    if judy_img is None or not person_keypoints_list:
        return display_frame

    frame_with_judy = display_frame.copy()

    for person_kpts in person_keypoints_list: # Iterate through all detected people
        # Keypoints needed for head: nose, left_eye, right_eye, (left_ear, right_ear optional)
        nose = person_kpts.get("nose")
        lefteye = person_kpts.get("left_eye")
        righteye = person_kpts.get("right_eye")
        # leftear = person_kpts.get("left_ear")
        # rightear = person_kpts.get("right_ear")

        if not nose: # Need at least a nose to anchor
            continue

        # Estimate head size
        head_width = 0
        if lefteye and righteye:
            head_width = abs(lefteye[0] - righteye[0])
        elif nose: # Fallback if eyes not detected, use a rough estimate
            # This is very approximate, maybe use shoulder width if available
            lshoulder = person_kpts.get("left_shoulder")
            rshoulder = person_kpts.get("right_shoulder")
            if lshoulder and rshoulder:
                head_width = abs(lshoulder[0] - rshoulder[0]) * 0.3 # Guess head is 30% of shoulder width
            else:
                head_width = 50 # Default pixel width if no other info

        if head_width == 0: head_width = 50 # Final fallback

        # Scale Judy's head based on detected head width
        judy_original_h, judy_original_w = judy_img.shape[:2]
        target_judy_w = int(head_width * head_scale_factor)
        if target_judy_w <=0: continue # Skip if target width is invalid

        scale_ratio = target_judy_w / judy_original_w
        target_judy_h = int(judy_original_h * scale_ratio)
        if target_judy_h <=0: continue

        resized_judy = cv2.resize(judy_img, (target_judy_w, target_judy_h), interpolation=cv2.INTER_AREA)

        # Calculate top-left position to place Judy's head
        # Center Judy's head around the nose, adjust slightly upwards
        center_x = nose[0]
        center_y = nose[1] - int(target_judy_h * 0.15) # Shift Judy's head up a bit relative to nose

        top_left_x = center_x - target_judy_w // 2
        top_left_y = center_y - target_judy_h // 2

        frame_with_judy = overlay_image_alpha(frame_with_judy, resized_judy, top_left_x, top_left_y)

    return frame_with_judy


def main():
    print("开始加载配置...")
    config = load_config()
    video_path = config['video_input_path']
    judy_head_path = config.get('judy_head_image_path')
    judy_head_scale = config.get('judy_head_scale_factor', 1.5)
    print(f"配置加载完成，视频路径: {video_path}")
    if judy_head_path:
        print(f"朱迪头像路径: {judy_head_path}, 缩放因子: {judy_head_scale}")

    # Load Judy Hopps head image (with alpha channel if PNG)
    judy_image = None
    if judy_head_path and os.path.exists(judy_head_path):
        judy_image = cv2.imread(judy_head_path, cv2.IMREAD_UNCHANGED)
        if judy_image is None:
            print(f"警告: 无法加载朱迪头像图片: {judy_head_path}")
        else:
            print(f"朱迪头像加载成功，尺寸: {judy_image.shape}")
            if judy_image.shape[2] < 4:
                print("警告: 朱迪头像图片没有Alpha通道，将尝试添加。为获得最佳效果，请使用带透明背景的PNG。")
    else:
        print(f"警告: 未找到朱迪头像图片路径或未配置: {judy_head_path}")


    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"错误: 无法打开视频 {video_path}"); return

    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"视频信息 - 宽度: {frame_width}, 高度: {frame_height}, FPS: {fps:.2f}, 总帧数: {total_frames if total_frames > 0 else 'N/A'}")

    out = None
    if config.get('video_output_path'):
        output_fps = fps if fps > 0 else 30
        print(f"创建输出视频: {config['video_output_path']} @ {output_fps:.2f} FPS")
        out = cv2.VideoWriter(config['video_output_path'], cv2.VideoWriter_fourcc(*'mp4v'), output_fps, (frame_width, frame_height))

    print("初始化姿势估计模块...")
    pose_module = PoseEstimator(config['yolo_pose_model_path'], config)
    print("初始化球追踪(KF)模块...")
    ball_module = BallTracker(config.get('tracknet_model_path', None), config)

    frame_num = 0
    start_time = time.time()
    print("开始处理视频...")
    while True:
        ret, frame = cap.read()
        if not ret:
            print("视频处理完成!"); break

        display_frame = frame.copy()

        # 1. Pose Estimation
        person_keypoints_list = pose_module.get_keypoints(frame)
        # Don't draw original keypoints on head if replacing head
        # display_frame = pose_module.draw_keypoints(display_frame, person_keypoints_list) # Or modify draw_keypoints to skip head

        # <<<<<<< HEAD REPLACEMENT >>>>>>>>
        if judy_image is not None:
            display_frame = replace_player_head(display_frame, person_keypoints_list, judy_image, judy_head_scale)
        # If Judy's head is NOT replaced, then draw the original keypoints
        if judy_image is None or not person_keypoints_list:
             display_frame = pose_module.draw_keypoints(display_frame, person_keypoints_list)


        # 2. Swing Classification
        swing_type = "No Swing"
        if person_keypoints_list:
            swing_type = pose_module.classify_swing(person_keypoints_list)

        # Ball Tracking
        raw_ball_detections = ball_module.predict_ball(frame)
        kf_estimated_ball_pos = ball_module.process_frame(raw_ball_detections, frame_num)

        # Annotation
        display_frame = ball_module.draw_ball(display_frame, kf_estimated_ball_pos)
        display_frame = ball_module.draw_trajectory(display_frame)
        display_frame = ball_module.draw_static_balls(display_frame)

        cv2.putText(display_frame, f"Swing: {swing_type}", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)
        if kf_estimated_ball_pos:
            ball_text = f"Ball (KF): ({int(kf_estimated_ball_pos[0])}, {int(kf_estimated_ball_pos[1])})"
            cv2.putText(display_frame, ball_text, (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)
        else:
            cv2.putText(display_frame, "Ball: Lost", (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2, cv2.LINE_AA)

        processing_time = time.time() - start_time
        current_fps = (frame_num + 1) / processing_time if processing_time > 0 else 0
        cv2.putText(display_frame, f"FPS: {current_fps:.1f}", (frame_width - 150, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

        cv2.imshow("Tennis Analysis", display_frame)
        if out: out.write(display_frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            print("用户按下'q'键，程序退出"); break
        frame_num += 1

    end_time = time.time()
    total_processing_time = end_time - start_time
    avg_fps = frame_num / total_processing_time if total_processing_time > 0 else 0
    print(f"处理结束. 总耗时: {total_processing_time:.2f} 秒. 平均 FPS: {avg_fps:.2f}")
    print("释放资源...")
    cap.release()
    if out: out.release()
    cv2.destroyAllWindows()
    print("程序结束")

if __name__ == "__main__":
    main()